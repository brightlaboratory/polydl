{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras 2.2.4/tensorflow 1.14.0 -> Version matching for local machine\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matmul/matmul3.c_1024_1024_32768_poly_perf.csv', 'matmul/matmul3.c_1024_16_500000_poly_perf.csv', 'matmul/matmul3.c_1024_32768_1024_poly_perf.csv', 'matmul/matmul3.c_128_2048_4096_poly_perf.csv', 'matmul/matmul3.c_2048_4096_32_poly_perf.csv', 'matmul/matmul3.c_320_3072_4096_poly_perf.csv', 'matmul/matmul3.c_32768_1024_1024_poly_perf.csv', 'matmul/matmul3.c_4096_4096_4096_poly_perf.csv']\n"
     ]
    }
   ],
   "source": [
    "myname = 'Gagandeep'\n",
    "\n",
    "#Using os.walk to walk through each file in the directory provided\n",
    "files = []\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('matmul/'):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        files.append(os.path.join(dirname, filename))\n",
    "\n",
    "# file_list = []\n",
    "# unique_file_list = []    \n",
    "\n",
    "\n",
    "# # For every file in the hierarchy this regex operation takes files only with N=1 or N=28\n",
    "\n",
    "# for value in files:\n",
    "#     x = re.search(\".*28_poly_perf.csv$\",value)\n",
    "# #     x = re.search(a,value)\n",
    "#     if(x):\n",
    "# #         print(x.group(0))\n",
    "#         file_list.append(x.group(0))\n",
    "#     y = re.search(\"\\w+/\\w+/\",value)\n",
    "#     if(y):\n",
    "#         if y.group(0) not in unique_file_list:\n",
    "#             unique_file_list.append(y.group(0))\n",
    "\n",
    "# #Sorts the ordering of files\n",
    "# file_list.sort()\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# For particular file we are generating various permutations \n",
    "arr = []\n",
    "for diff_file in files:\n",
    "# for layer in range(1,20):\n",
    "    train_df = pd.read_csv(diff_file,header=None)\n",
    "    train_df = train_df.drop([0,2,3,4,5,6,7,8,9], axis=1)\n",
    "\n",
    "    train_df = train_df.sort_values(by=[1],ascending=False)\n",
    "\n",
    "    train_df = train_df.dropna(axis=1)\n",
    "    train_df[13]=0;\n",
    "    \n",
    "    npdata = np.array(train_df)\n",
    "    \n",
    "    for i in range(len(npdata)):\n",
    "        for j in range(len(npdata)):\n",
    "            arr.append(np.concatenate((npdata[i],npdata[j]), axis=0))\n",
    "arr = np.asarray(arr)\n",
    "\n",
    "ver_2_GF = npdata.shape[1]\n",
    "\n",
    "\n",
    "# For each data point we are creating the output as 01 or 10 and concatenating with the file\n",
    "out1 = []\n",
    "out2 = []\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    if arr[i][0] > arr[i][ver_2_GF]:\n",
    "        out1.append(1)\n",
    "        out2.append(0)\n",
    "\n",
    "    else:\n",
    "        out1.append(0)\n",
    "        out2.append(1)\n",
    "\n",
    "arr = pd.DataFrame(arr)\n",
    "arr = arr.drop([0,ver_2_GF],axis=1)\n",
    "arr[\"sum\"] = arr.sum(axis=1)\n",
    "arr = arr.loc[:,[1,2,3,4,6,7,8,9]].div(arr[\"sum\"], axis=0)\n",
    "arr.head()\n",
    "\n",
    "out1 = pd.DataFrame(out1)\n",
    "out2 = pd.DataFrame(out2)\n",
    "\n",
    "arr = pd.concat([arr, out1, out2,], axis=1, sort=False)\n",
    "arr = np.array(arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.6880 - accuracy: 0.5262\n",
      "Epoch 2/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.6583 - accuracy: 0.6021\n",
      "Epoch 3/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.6291 - accuracy: 0.6395\n",
      "Epoch 4/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5825 - accuracy: 0.6851\n",
      "Epoch 5/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5661 - accuracy: 0.6974\n",
      "Epoch 6/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5588 - accuracy: 0.7025\n",
      "Epoch 7/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5547 - accuracy: 0.7052\n",
      "Epoch 8/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5506 - accuracy: 0.7081\n",
      "Epoch 9/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5482 - accuracy: 0.7095\n",
      "Epoch 10/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5462 - accuracy: 0.7107\n",
      "Epoch 11/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5439 - accuracy: 0.7123\n",
      "Epoch 12/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5425 - accuracy: 0.7132\n",
      "Epoch 13/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5400 - accuracy: 0.7150\n",
      "Epoch 14/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5389 - accuracy: 0.7157\n",
      "Epoch 15/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5374 - accuracy: 0.7167\n",
      "Epoch 16/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5361 - accuracy: 0.7171\n",
      "Epoch 17/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5343 - accuracy: 0.7183\n",
      "Epoch 18/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5332 - accuracy: 0.7188\n",
      "Epoch 19/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5322 - accuracy: 0.7195\n",
      "Epoch 20/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5305 - accuracy: 0.7204\n",
      "Epoch 21/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5296 - accuracy: 0.7210\n",
      "Epoch 22/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5288 - accuracy: 0.7210\n",
      "Epoch 23/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5266 - accuracy: 0.7223\n",
      "Epoch 24/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5262 - accuracy: 0.7225\n",
      "Epoch 25/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5244 - accuracy: 0.7234\n",
      "Epoch 26/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5239 - accuracy: 0.7239\n",
      "Epoch 27/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5233 - accuracy: 0.7243\n",
      "Epoch 28/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5214 - accuracy: 0.7253\n",
      "Epoch 29/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5208 - accuracy: 0.7252\n",
      "Epoch 30/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5196 - accuracy: 0.7262\n",
      "Epoch 31/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5187 - accuracy: 0.7265\n",
      "Epoch 32/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5175 - accuracy: 0.7271\n",
      "Epoch 33/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5173 - accuracy: 0.7274\n",
      "Epoch 34/40\n",
      "12147/12147 [==============================] - 20s 2ms/step - loss: 0.5119 - accuracy: 0.7306\n",
      "Epoch 40/40\n",
      "12147/12147 [==============================] - 21s 2ms/step - loss: 0.5115 - accuracy: 0.7309\n",
      "32535/32535 [==============================] - 27s 826us/step - loss: 0.5141 - accuracy: 0.7303\n",
      "Accuracy: 73.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Till Here I have preprocessed the data properly.\n",
    "# Splitting the data into 70% training and 30% test data.\n",
    "np.random.shuffle(arr)\n",
    "length = int(0.70*(len(arr)))\n",
    "training_set = arr[:length]\n",
    "test_set = arr[length:]\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Dense(32, input_dim=8, activation='relu'))\n",
    "# model.add(Dense(20, activation='relu'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(12, activation='softsign'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.add(Dense(64, input_dim=8, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(16, activation='softsign'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(training_set[:,:-2],training_set[:,8:], epochs=40, batch_size=200)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(test_set[:,:-2],test_set[:,8:])\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# Again for a file we are making all permutations and are passing through Model for prediction.\n",
    "for diff_file in files:\n",
    "# for layer in range(1,20):\n",
    "    arr_predict = []\n",
    "    train_df = pd.read_csv(diff_file,header=None)\n",
    "    \n",
    "    myname = \"results_\"+diff_file\n",
    "    train_df = train_df.sort_values(by=[1],ascending=False)\n",
    "    print_ver_GF = train_df.dropna(axis=1)\n",
    "    print_ver_GF = np.asarray(print_ver_GF)\n",
    "#     print(print_ver_GF[0,1])\n",
    "    train_df = train_df.drop([0,1,2,3,4,5,6,7,8,9], axis=1)\n",
    "    train_df = train_df.dropna(axis=1)\n",
    "    train_df[13]=0;\n",
    "    npdata = np.array(train_df)\n",
    "    \n",
    "    for i in range(len(npdata)):\n",
    "        for j in range(len(npdata)):\n",
    "            arr_predict.append(np.concatenate((npdata[i],npdata[j]), axis=0))\n",
    "    arr_predict = pd.DataFrame(arr_predict) \n",
    "    arr_predict[\"sum\"] = arr_predict.sum(axis=1)\n",
    "    arr_predict = arr_predict.loc[:,[0,1,2,3,4,5,6,7]].div(arr_predict[\"sum\"], axis=0)\n",
    "    arr_predict = np.asarray(arr_predict)\n",
    "    prediction = model.predict(arr_predict)\n",
    "\n",
    "\n",
    "# Uptill here we have the predicted output data ready.\n",
    "# Now we calculate number of wins for each data point and append (ActualRank, GFlops, Version, wins,\n",
    "# PolyRank) values in \"print_result\" list, for each file\n",
    "\n",
    "    version =0\n",
    "    flag=0\n",
    "    count_win =0\n",
    "    distinct_wins = []\n",
    "    Actual_rank = 0\n",
    "    print_result = []\n",
    "#     for x in prediction:\n",
    "#         print(x)\n",
    "#     break\n",
    "    for values in prediction[:,0]:\n",
    "        if flag<len(npdata):\n",
    "            flag +=1\n",
    "            if values>0.7:\n",
    "                count_win+=1\n",
    "        else:\n",
    "            print_result.append([Actual_rank+1,print_ver_GF[Actual_rank,1],print_ver_GF[Actual_rank,0],count_win])\n",
    "            Actual_rank+=1\n",
    "            version+=1\n",
    "            if count_win not in distinct_wins:\n",
    "                distinct_wins.append(count_win)\n",
    "            count_win=0\n",
    "            flag=1\n",
    "            if values>0.7:\n",
    "                count_win+=1\n",
    "    print_result.append([Actual_rank+1,print_ver_GF[Actual_rank,1],print_ver_GF[Actual_rank,0],count_win])\n",
    "    if count_win not in distinct_wins:\n",
    "        distinct_wins.append(count_win)\n",
    "    \n",
    "# On the basis of Number of wins, We are calculating the number of Polyrank for each data point.\n",
    "    distinct_wins.sort(reverse = True)\n",
    "    polyrank = []\n",
    "    for values in print_result:\n",
    "        polyrank.append(distinct_wins.index(values[3])+1)\n",
    "    polyrank = pd.DataFrame(polyrank)\n",
    "    Final = pd.concat([pd.DataFrame(print_result),polyrank], axis=1, sort=False)\n",
    "    Final= np.asarray(Final)\n",
    "\n",
    "    temp = Final[Final[:,4] == 1]\n",
    "\n",
    "    num_row_in_top5_per = math.ceil((Final.shape[0])*0.05)\n",
    "    rank = 1\n",
    "\n",
    "# Calculating best in Top 5%\n",
    "    top5_max_list =[]    \n",
    "    while num_row_in_top5_per>0:\n",
    "        temp_rank = Final[Final[:,4] == rank]\n",
    "        top5_max_list.append(np.max(temp_rank[:,1]))\n",
    "        num_row_in_top5_per-=temp_rank.shape[0]\n",
    "        rank+=1\n",
    "        \n",
    "    top5_perf_list = [np.max(Final[:,1]),np.max(temp[:,1]),Final.shape[0],max(top5_max_list),np.min(Final[:,1]),np.median(Final[:,1])]\n",
    "#     print(\"Hello\",top5_perf_list)\n",
    "\n",
    "# Writing the Wins, polyrank etc to a file.\n",
    "    f = open(myname, \"a+\") \n",
    "    f.write('ActualRank, GFlops, Version, wins, PolyRank,\\n')\n",
    "    f.close()\n",
    "    for values in Final:\n",
    "        f = open(myname, \"a+\")        \n",
    "        str_temp = \"\"\n",
    "        for val in values:\n",
    "            str_temp = str_temp + str(val)+\", \"\n",
    "        f.write(\"%s\\n\" % str_temp)\n",
    "        f.close()\n",
    "\n",
    "# Creating summary files  and writing it to a file\n",
    "    myname_summary = \"summary_results_matmul\"\n",
    "#     string2 = re.search(\"\\w+/\\w+/\",diff_file).group(0)\n",
    "#     if string2 in diff_file:\n",
    "#         string2 = diff_file.replace(string2,'')\n",
    "#     if \"_poly_perf.csv\" in string2:\n",
    "#         string2 = string2.replace(\"_poly_perf.csv\",'')\n",
    "        \n",
    "    f = open(myname_summary, \"a\")\n",
    "#     f.write(\"Max_GFLOPS, Poly_Top_1GFLOPS,numVariants,Poly_Top_0.050000,Min_GFLOPS,Median_GFLOPS,\\n\")\n",
    "    string2=diff_file\n",
    "    str_temp = string2+\", \"\n",
    "    for val in top5_perf_list:\n",
    "        str_temp = str_temp + str(val) + \", \"\n",
    "    f.write(\"%s\\n\" % str_temp)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
